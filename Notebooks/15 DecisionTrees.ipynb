{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6994fd2",
   "metadata": {},
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "Un árbol de decisión es un árbol cuyos nodos representan valores de los rasgos de los datos; los hijos de los nodos responden a decisiones sobre estos valores y las hojas del árbol son clases a las que pueden pertenecer los datos.\n",
    "\n",
    "Aquí presentamos una implementación del algoritmo para aprender árboles de decisión con base en ganancia de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0100c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dd3d5",
   "metadata": {},
   "source": [
    "### Modelo para árboles de decisión\n",
    "\n",
    "En primer lugar definimos una clase para crear objetos nodos que puedan usarse para construir el árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5810bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decisionnode:\n",
    "    \"\"\"\n",
    "    Clase para guardar nodos del árbol de decisión.\n",
    "    \"\"\"\n",
    "    def __init__(self,feat=-1,value=None,results=None,tb=None,fb=None):\n",
    "        #Columna\n",
    "        self.feat=feat\n",
    "        #Valor\n",
    "        self.value=value\n",
    "        #Resultados\n",
    "        self.results=results\n",
    "        #Rama positiva\n",
    "        self.tb=tb\n",
    "        #Rama negativa\n",
    "        self.fb=fb\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.results != None:\n",
    "            return \"Class: {}\".format(self.results)\n",
    "        else:\n",
    "            return \"Column {}: >={}?\".format(self.feat, self.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f9039d",
   "metadata": {},
   "source": [
    "También definimos funciones que utilizaremos con los árboles de decisión: una función que nos permitirá particionar los datos en base a si cumplen con cierto valor de un rasgo.\n",
    "\n",
    "Una función para contar las apariciones de un valor a lo largo de los diferentes ejemplos en los datos.\n",
    "\n",
    "Finalmente, definimos una función que nos permitirá visualizar el árbol de decisión y las decisiones que se hacen para llegar a una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bd5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideSet(X,column,value):\n",
    "    \"\"\"\n",
    "    Función para separar el conjunto de datos bi-particionando.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    rows : array\n",
    "        Renglones de los datos\n",
    "    column : array\n",
    "        Columnas de los datos\n",
    "    value : x\n",
    "        Valor actual de los datos.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Bi-partición de los datos.\n",
    "    \"\"\"\n",
    "    split_function=None\n",
    "    #Si el valor es número\n",
    "    if isinstance(value,int) or isinstance(value,float):\n",
    "        #La partición se hace cuando se supera el valor\n",
    "        split_function = lambda row: row[column] >= value\n",
    "    else:\n",
    "        #Si no, la aprtición se hace si tiene el valor\n",
    "        split_function = lambda row: row[column] == value\n",
    "\n",
    "    #Conjunto uno cumple con el rasgo\n",
    "    set1 = [i for i,row in enumerate(X) if split_function(row)]\n",
    "    #Conjunto dos no cumple con el rasgo\n",
    "    set2 = [j for j,row in enumerate(X) if not split_function(row)]\n",
    "\n",
    "    return set1, set2\n",
    "\n",
    "def uniqueCounts(rows):\n",
    "    \"\"\"\n",
    "    Función que cuenta las apariciones de un valor en los renglones.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    rows : array\n",
    "        Renglones en los datos de entrada\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Diccionario de valores y sus frecuencias\n",
    "    \"\"\"\n",
    "    #Guarda resultados\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        #revisa el valor anterior del renglón\n",
    "        r = row[len(row) - 1]\n",
    "        #print(r, len(row) - 1)\n",
    "        #Si no esta en resultados\n",
    "        if r not in results: \n",
    "            #Guarda el resultado actual\n",
    "            results[r] = 0\n",
    "        #Suma uno al resultado de r\n",
    "        results[r] += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "def print_tree(tree, indent = '\\t'):\n",
    "    \"\"\"\n",
    "    Funciín para imprimer el árbol de forma jerárquica.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    tree : object\n",
    "        Árbol que se desea visualizar\n",
    "    \"\"\"\n",
    "    # Imprime el resultado/clase\n",
    "    if tree.results != None:\n",
    "        print( list(tree.results.keys())[0] )\n",
    "    else:\n",
    "        #Imprime árbol\n",
    "        print(tree)\n",
    "        #Imprime ramas\n",
    "        print(indent + 'True->', end=\" \")\n",
    "        print_tree(tree.tb, indent=indent+'\\t')\n",
    "        print(indent + 'False->', end=\" \")\n",
    "        print_tree(tree.fb, indent=indent+'\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83da97a",
   "metadata": {},
   "source": [
    "Definimos además dos funciones que servirán para calcular la relevancia de las particiones realizadas:\n",
    "\n",
    "* Entropía: $H(X_k) = \\sum_{y\\in X_k} p(y) \\log p(y)$\n",
    "* Gini Impurity: $G(X_k) = \\sum_{y\\in X_k} p(y)\\big(1-p(y)\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba776a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(classes):\n",
    "    \"\"\"\n",
    "    Función para cálculo de la entropía.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    rows : array\n",
    "        Renglones de datos.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Entropía total estimada.\n",
    "    \"\"\"\n",
    "    #Realiza conteos\n",
    "    results = Counter(classes)\n",
    "    #Guarda la entorpía\n",
    "    H = 0.0\n",
    "    for r in results.keys():\n",
    "        #Calcula probabilidades\n",
    "        p = results[r]/len(classes)\n",
    "        #Calcula entropía\n",
    "        H -= p*np.log2(p)\n",
    "\n",
    "    return H\n",
    "\n",
    "def giniimpurity(classes):\n",
    "    \"\"\"\n",
    "    Función para cálculo de la impureza de Gini.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    rows : array\n",
    "        Renglones de datos.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Impureza total estimada.\n",
    "    \"\"\"\n",
    "    #Total de clases en nodo\n",
    "    total = len(classes)\n",
    "    #Frecuencia de clases\n",
    "    counts = Counter(classes)\n",
    "    #Gini impurity\n",
    "    Gini = 0\n",
    "    for y, frec in counts.items():\n",
    "        #Probabilidad de clase\n",
    "        p = frec/total\n",
    "        #Suma valor de impureza\n",
    "        Gini += p*(1-p)\n",
    "\n",
    "    return Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fc08c",
   "metadata": {},
   "source": [
    "Ahora definimos la clase para el modelo del árbol de decisión. Para construir el árbol de decisión se biparticionan los datos y se calcula la ganancia de información de esa bipartición:\n",
    "\n",
    "$$IG(X,\\phi) = H(X)-\\mathbb{E}_{p\\sim \\phi}H(X|\\phi)$$\n",
    "\n",
    "Donde $\\phi$ es un rasgo y $X$ son los datos actuales. En cada paso, se calcula la ganancia de información y se elige como nodo el rasgo $\\phi$ que mayor ganancia aporte. Sus hijos responderán a los datos que tienen el valor del rasgo y aquellos que no. Se concluye el algoritmo hasta que todos los hijos pertenezcan a datos de una sola clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0a8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Clase para construcción del algoritmo de árbol de decisión.\n",
    "    \"\"\"\n",
    "    def __init__(self, score=entropy):\n",
    "        #El score con el que se calculará\n",
    "        #la ganancia\n",
    "        self.score = score\n",
    "        #El árbol que se ha obtenido\n",
    "        self.tree = None\n",
    "\n",
    "    def buildTree(self,X,Y):\n",
    "        \"\"\"\n",
    "        Función que construye el árbol de decisión en base a los datos.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        X,Y : array\n",
    "            Dataset supervisado para entrenar y construir el árbol.\n",
    "        \"\"\"\n",
    "        #Tamaño y dimensión d de los datos\n",
    "        n,d = X.shape\n",
    "        #Calcula el valor de entropía o impureza\n",
    "        current_score = self.score(Y)\n",
    "\n",
    "        #Guarda ganancia\n",
    "        best_gain = 0.0\n",
    "        #Guarda mejor criterio\n",
    "        best_criteria = None\n",
    "        #Guarda conjuntos\n",
    "        best_sets = None\n",
    "        #Guarda las clases\n",
    "        best_classes = None\n",
    "        for feature in range(0, d):\n",
    "            #Guarda valores de columnas\n",
    "            feature_values = set([x[feature] for x in X])\n",
    "\n",
    "            for value in feature_values:\n",
    "                #Separa los conjuntos\n",
    "                set1, set2 = divideSet(X, feature, value)\n",
    "                #Calcula las probabilidades\n",
    "                p = float(len(set1))/len(X)\n",
    "                #Calcula la ganancia: Score - E[Score(sets)]\n",
    "                gain = current_score-p*self.score(Y[set1])-(1-p)*self.score(Y[set2])\n",
    "                #print(value, gain,self.score(Y[set1]), self.score(Y[set2]))\n",
    "\n",
    "                #Si la ganancia actual es mejor\n",
    "                if gain > best_gain and len(set1) > 0 and len(set2) > 0:\n",
    "                    #La mejor ganancia es la actual\n",
    "                    best_gain = gain\n",
    "                    #Criterios son el valor de la columna\n",
    "                    best_criteria = (feature, value)\n",
    "                    #Guardas los mejores conjuntos\n",
    "                    best_sets = (X[set1],X[set2])\n",
    "                    #Clases\n",
    "                    best_classes = (Y[set1],Y[set2])\n",
    "\n",
    "        #Rvisa si la ganancia es mayor a 0\n",
    "        if best_gain > 0:\n",
    "            #Rama con valor 1 o T\n",
    "            trueBranch = self.buildTree(best_sets[0], best_classes[0])\n",
    "            #Rama con valor 0 o F\n",
    "            falseBranch = self.buildTree(best_sets[1], best_classes[1])\n",
    "\n",
    "            #Genera el nodo con los valores dados\n",
    "            return decisionnode(feat = best_criteria[0], value = best_criteria[1],\n",
    "                    tb = trueBranch, fb = falseBranch)\n",
    "        #En otro caso\n",
    "        else: \n",
    "            return decisionnode(results = Counter(Y))\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        \"\"\"\n",
    "        Función para entrenar el árbol de decisión.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        X,Y : array\n",
    "            Dataset supervisado para entrenar y construir el árbol.\n",
    "        \"\"\"\n",
    "        #Construye y guarda el árbol\n",
    "        self.tree = self.buildTree(X,Y)\n",
    "        \n",
    "    def predict(self, observation, subtree=None):\n",
    "        \"\"\"\n",
    "        Función para predecir la clase según el árbol.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        observation : array\n",
    "            Datos que se van a clasificar\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "            Clase predica por el árbol de decisión.\n",
    "        \"\"\"\n",
    "        #Revisa si el árbo ha sido entendo\n",
    "        if self.tree == None:\n",
    "            raise Exception('Debe entrenarse el árbol. Usar método fit(x,y)')\n",
    "        #Revisa cuál es el subárbol\n",
    "        if subtree == None:\n",
    "            tree = self.tree\n",
    "        else:\n",
    "            tree = subtree\n",
    "        \n",
    "        if tree.results != None:\n",
    "            #Regresa la clase final\n",
    "            return list(tree.results.keys())[0]\n",
    "        else:\n",
    "            #Crea el ramaje para llegar a la clase\n",
    "            v = observation[tree.feat]\n",
    "            branch = None\n",
    "            if isinstance(v, int) or isinstance(v, float):\n",
    "                if v >= self.tree.value: \n",
    "                    branch = tree.tb\n",
    "                else: \n",
    "                    branch = tree.fb\n",
    "            else:\n",
    "                if v == tree.value: \n",
    "                    branch = tree.tb\n",
    "                else: \n",
    "                    branch = tree.fb\n",
    "\n",
    "        return self.predict(observation, branch)\n",
    "    \n",
    "    def draw_tree(self):\n",
    "        #Imprime el árbol\n",
    "        print_tree(self.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c4d6d",
   "metadata": {},
   "source": [
    "## Aplicación del modelo\n",
    "\n",
    "Podemos aplicar el modelo de árbol de decisión a datos simples donde tengamos valores categóricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a731b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0: >=no tos?\n",
      "\tTrue-> no gripe\n",
      "\tFalse-> Column 1: >=fiebre?\n",
      "\t\tTrue-> gripe\n",
      "\t\tFalse-> no gripe\n"
     ]
    }
   ],
   "source": [
    "#Datos de entrenamiento\n",
    "x = np.array([['tos','fiebre', 'mareo'],\n",
    "       ['no tos','fiebre','mareo'],\n",
    "       ['tos','no fiebre','no mareo'],\n",
    "       ['tos','fiebre','no mareo']])\n",
    "#Clases de los datos\n",
    "y = np.array(['gripe', 'no gripe', 'no gripe', 'gripe'])\n",
    "\n",
    "#Creación del modelo\n",
    "model = DecisionTree()\n",
    "model.fit(x,y)\n",
    "\n",
    "#Visualización del árbol\n",
    "model.draw_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c972ad",
   "metadata": {},
   "source": [
    "### Un ejemplo más complejo\n",
    "\n",
    "Tomamos los datos de iris de Sklearn que buscan predecir la clase de hojas. En este caso preparamos los datos y generamos el árlbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "476e45ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 2: >=3.5?\n",
      "\tTrue-> Column 2: >=4.8?\n",
      "\t\tTrue-> Column 2: >=5.1?\n",
      "\t\t\tTrue-> 2\n",
      "\t\t\tFalse-> Column 0: >=6.7?\n",
      "\t\t\t\tTrue-> 1\n",
      "\t\t\t\tFalse-> Column 1: >=3.2?\n",
      "\t\t\t\t\tTrue-> 1\n",
      "\t\t\t\t\tFalse-> 2\n",
      "\t\tFalse-> Column 0: >=5.0?\n",
      "\t\t\tTrue-> 1\n",
      "\t\t\tFalse-> 2\n",
      "\tFalse-> 0\n"
     ]
    }
   ],
   "source": [
    "#Separación de los datoa\n",
    "x_train, x_test, y_train, y_test = train_test_split(load_iris().data, load_iris().target, test_size=0.3)\n",
    "\n",
    "#Creación y entrenamiento\n",
    "tree = DecisionTree(score=entropy)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "#Visualización del árbol\n",
    "tree.draw_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e770bb7",
   "metadata": {},
   "source": [
    "Finalmente, evaluamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96b0fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92        18\n",
      "           1       0.00      0.00      0.00        14\n",
      "           2       0.54      1.00      0.70        13\n",
      "\n",
      "    accuracy                           0.69        45\n",
      "   macro avg       0.47      0.67      0.54        45\n",
      "weighted avg       0.50      0.69      0.57        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mijangos/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Predicción sobre dataset de evaluación\n",
    "y_pred = [tree.predict(x) for x in x_test]\n",
    "#Evaluación\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df12ef7",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
